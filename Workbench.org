*Workbench使用说明*

** 1. 运行说明
运行环境需要 java 1.8 版本.

  1. 解压 workbench.zip后, 会出现如下内容:
    - server-0.0.1-SNAPSHOT.jar 文件
    - Guandata_Workbench 目录
    其中, server-*.jar文件是运行程序文件, 而Guandata_Workbench是一些自定义的配置文件. 2个文件/文件夹需要放在同一个目录
  2. 进入命令行下运行如下命令, 来启动workbench程序
     #+BEGIN_SRC sh
java -jar server-0.0.1-SNAPSHOT.jar
     #+END_SRC
     如果机器的内存比较小(比如2G左右), 则可以加上限制JVM所用内存的参数:
     #+BEGIN_SRC sh
java -Xmx800m -jar server-0.0.1-SNAPSHOT.jar
     #+END_SRC
     本例中是限制java heap内存的最大使用量为 800 MB

** 2. 配置文件说明
在 Guandata_Workbench 根目录下,有一个配置文件: settings.json 文件, 下面将分节来介绍各个部分

#+BEGIN_SRC javascript
{
  "guandata": {
    "homeUrl": "http://www.guandata.com:9000",
    "domain": "demo1",
    "email": "__change_to_your_guandata_email__",
    "password": "__change_to_your_guandata_passwod__"
  },
  "input_parameter": {
    "today_date": "new org.joda.time.LocalDate().toString('yyyy-MM-dd')",
    "yesterday_date": "new org.joda.time.LocalDate().minusDays(1).toString('yyyy-MM-dd')"
  },
  "schedule_cron": {
    "daily": "0+0+5+*+*+?"
  },
  "mysql": {
    /*Database Driver class, noramlly this is not changed*/
    "driverClassName": "com.mysql.jdbc.Driver",
    /*url can also pass in some config parameters*/
    "url": "jdbc:mysql://127.0.0.1:3306/your_db_name?verifyServerCertificate=false&useSSL=true",
    "username": "__change_to_your_db_user_name__",
    "password": "__change_to_your_db_user_password__",
    /*还可以继续添加数据库支持的其它配置property*/
    "defaultFetchSize": "5000",
    "useCursorFetch": "true"
  },
  "postgresql": {
    "driverClassName": "org.postgresql.Driver",
    "url": "jdbc:postgresql://127.0.0.1:5432/postgres",
    "username": "__change_to_your_db_user_name__",
    "password": "__change_to_your_db_user_password__"
  },
  "greenplum": {
    "driverClassName": "org.postgresql.Driver",
    "url": "jdbc:postgresql://127.0.0.1:5432/postgres",
    "username": "__change_to_your_db_user_name__",
    "password": "__change_to_your_db_user_password__"
  }
}
#+END_SRC

Note: 我们在JSON的基础上加入了 "注释"支持: 可以把注释写在 /* */ 中. 其它的应该都是标准JSON. 所以, 需要额外注意该配置文件写的是否合法JSON:
  1. 当JSON中的参数是数组时, 注意最后一个元素后面不能有多余的逗号, 比如:
     #+BEGIN_SRC javascript
["a1", "a2", "a3",]
     #+END_SRC
     这时, a3 后面的逗号会导致文件出错
  2. 当JSON的参数是Object时, 最后一个元素后面也不能有多余的逗号, 比如:
     #+BEGIN_SRC javascript
{
    "key1": "value1",
    "key2": "value2",
    "key3": "value3",
}
     #+END_SRC
     这里的 value3 后面的多余的逗号, 也会导致出错.

*Note*: 目前主要是因为我们底层使用的 jackson 库: https://github.com/FasterXML/jackson 还不支持这些 "Trailing_Comma", 但是在其下一个版本(2.9)中, 将会支持, 到时我们就不会有上面的这么容易出错的2个"多余的逗号"的问题了

*** 2.1 guandata 部分
本部分是配置文件中的:
#+BEGIN_SRC javascript
{
  "guandata": {
    "homeUrl": "http://www.guandata.com:9000",
    "domain": "demo1",
    "email": "__change_to_your_guandata_email__",
    "password": "__change_to_your_guandata_passwod__"
  }
}
#+END_SRC

主要是用户在观数的登陆信息:
  - homeUrl: 观数平台的API地址, 一般用户不需要改变
  - domain: 观数上的"公司域名"
  - email: 观数上登陆的email名字
  - password: 观数上该用户的密码

这里是基本信息, 比较重要的一点是: 输入的用户需要具有"管理员权限", 否则, 会导致无法创建data source

*** 2.2 JDBC数据源部分
本部分是数据库相关部分. 目前主要支持3种数据库 (如果用户有其它jdbc数据库的需要, 我们可以快速添加):
  - mysql
  - postgresql
  - greenplum

每一个都对应于配置文件中的一小块, 比如: greenplum数据库:
#+BEGIN_SRC javascript
{
  "greenplum": {
    "driverClassName": "org.postgresql.Driver",
    "url": "jdbc:postgresql://127.0.0.1:5432/postgres",
    "username": "__change_to_your_db_user_name__",
    "password": "__change_to_your_db_user_password__"
  }
}
#+END_SRC

其中:
  - driverClassName: jdbc driver中的java类名, 一般无需修改
  - url: 数据库的链接地址
  - username: 数据库的用户名
  - password: 数据库的密码

*** 2.3 定时调度
对于JDBC数据源, 我们一般都是配置为定时任务, 每天夜里导入前一天的数据. 那么需要我们自定义"定时任务".

*NOTE*: 本程序中假设客户的机器上的时区已经设置为正确的本地时间, 比如: 对于中国的公司, 一般机器上都设置为了"北京时间", 以及的定时任务配置时, 就是采用的机器的本地时间

对应于配置文件中的"schedule_cron"部分
#+BEGIN_SRC javascript
{
  "schedule_cron": {
    "daily": "0+0+5+*+*+?"
  }
}
#+END_SRC

本例中只展示了 1个定时出发器, 其名字是"daily", 而其调度时间为: 0+0+5+*+*+?  代表着: 每天凌晨5点触发一次.

其中"0+0+5+*+*+?"的语法是类似于CRON任务的一种写法, 具体可以参考: http://camel.apache.org/quartz2.html 

当然, 定时的需求很多, 用户可以再额外自定义自己的任务的触发时间, 比如:

#+BEGIN_SRC javascript
{
  "schedule_cron": {
    "daily": "0+0+7+*+*+?",
    "daily_noon": "0+10+12+*+*+?",
    "hourly": "0+10+*+*+*+?",
    "weekly": "0+10+5+?+*+MON"
  }
}
#+END_SRC

则建了如下的时间触发器:
  1. daily, 改为每天7点执行
  2. daily_noon, 改为每天中午12:10执行
  3. hourly, 每小时的10分钟时执行 (触发次数会比较多, 需要谨慎配置)
  4. weekly, 每周一的早上5:10执行

具体的使用方法请参考下面的案例

*** 2.4 SQL模板中需要的输入参数
对于JDBC数据源, 我们有了"定时调度", 但是有个问题, 每天的sql是跟时间有关的, 我们怎么才能配置每天导入的数据不一样呢? 这时, 需要我们的 "input_parameter"

#+BEGIN_SRC javascript
{
  "input_parameter": {
    "today_date": "new org.joda.time.LocalDate().toString('yyyy-MM-dd')",
    "yesterday_date": "new org.joda.time.LocalDate().minusDays(1).toString('yyyy-MM-dd')"
  }
}
#+END_SRC

这些参数也是自定义的. 比如: 本例子中, 我们定义了2个时间相关的模板参数:
  - today_date
  - yesterday_date

比如: 今天是 2017年3月13日, 那么这两个参数的值将会自动算为:
  - today_date 为:  2017-03-13
  - yesterday_date 为: 2017-03-12

这个时候, 在SQL文件中, 我们只需要配置为如下SQL
#+BEGIN_SRC sql
select customer_id, max(customer_name), sum(sales)
from sales_orders
where order_date = '${yesterday_date}'
group by customer_id
#+END_SRC

在运行该sql时, ${yesterday_date} 将会被替换为 2017-03-12

当然, 日期的格式也可以定制: 比如, 公司的数据库中是用的类似于 "20170312"的格式来存储的日期, 那么我们可以配置为:
#+BEGIN_SRC javascript
{
  "input_parameter": {
    "yesterday_date_short": "new org.joda.time.LocalDate().minusDays(1).toString('yyyyMMdd')"
  }
}
#+END_SRC

那么这个参数的格式就是 "20170312"这种格式了 (唯一修改的地方是: 把日期的格式化string由 'yyyy-MM-dd' 替换为 'yyyyMMdd'

** 3. GreenPlum 实际示例
*** 3.1 定义调度的数据表
本次我们要导入的 "每天订单分析"数据是存储在 greenplum 数据库中, 并且该数据是每天早上5点钟来更新的. 那么, 我们可以首先在 Guandata_Workbench的 greenplum 子目录下, 先创建一个 命名为 "每天订单分析" 的子目录. 因为我们的 settings.json 文件中, 已经配置了如下的"定时器":

#+BEGIN_SRC javascript
{
  "schedule_cron": {
    "daily_07": "0+0+5+*+*+?"
  }
}
#+END_SRC

那么我们在 "Guandata_Workbench/greenplum/每天订单分析" 目录下, 建立一个 daily_order_analysis.daily_07.sql 的文件: 
#+BEGIN_SRC sql
select order_date, customer_id, max(customer_name), sum(sales)
from sales_orders
where order_date = '${yesterday_date}'
group by order_date, customer_id
#+END_SRC

对于文件名来说, "daily_order_analysis.daily_07.sql"的 "daily_order_analysis"部分不重要, 重要的是其后缀: ".daily_07.sql". 对于由 "daily_07": "0+0+5+*+*+?" 来触发的定时器, 其实现是: 当触发执行时, 自动遍历所有的以 ".daily_07.sql"结尾的sql, 并执行.

*NOTE: 这一步中一定要注意"数据表"的名字不会和现有表冲突. Workbench上传数据时, 会根据"数据表"名字来判断其是否已经存在, 如果已经存在, 则只做更新操作, 不会再新建表*

*** 3.2 配置主键 (去重)
因为定时任务执行是, 每天都会更新数据, 这样就会有以下情况出现:
  1. 对于同一个数据源, 有时候数值统计的sql有问题, 需要重新跑一下来覆盖之前的数据
  2. 数据在每天的上传时会有叠加的部分(比如: 每天都是统计前一周的每日数据)

默认不做处理时, 有可能会出现, 同样的一条数据上传了多次的情况, 造成数据统计出问题. 

解决方法: 指定某些列为"主键", 这样, 如果上传了2次数据, 系统也会根据"主键"来把重复的数据"去重".  注意: 被定义为主键的列, 其值不能为NULL 或者空字符串. ( Apache Cassandra的限制)

具体做法: 在数据源的表(本例中是"daily_order_analysis")目录下, 增加该数据源专有的 settings.json, 其内容是:
#+BEGIN_SRC javascript
{
    "table": {
        "columns": [
            {"name": "order_date", "isPrimaryKey": true},
            {"name": "customer_id", "isPrimaryKey": true}
        ]
    }
}
#+END_SRC
来表明, 本表中, "order_date"和"customer_id"来作为"去重"

本节配置好了以后, 即使同样的sql执行多次, 其结果也不会有重复.

*** 3.3 补之前的数据
有了上面2步, 我们就可以等明天触发器调度时, 来自动导入前一天的数据了. 但是我们经常还需要把之前的一段时间的历史数据导入到系统中. 那么, 做法也很简单, 在相同目录下(本例子中为:  "Guandata_Workbench/greenplum/每天订单分析"), 新建一个 "fill_data.once.sql", 其内容是:

#+BEGIN_SRC sql
select order_date, customer_id, max(customer_name), sum(sales)
from sales_orders
where order_date >= '2017-01-01' and order_date <= '2017-03-12'
group by order_date, customer_id
#+END_SRC

系统会马上把之前的数据补好

*提示: 需要在输入完所有的sql后, 再保存为 .once.sql*, 因为, 系统会时时刻刻监视着这些目录下的 以".once.sql"结尾的文件, 如果发现了, 则马上会执行该sql. 如果执行完, 会把这个文件的后缀由 ".once.sql" 改为: ".once.sql_FINISHED" 或者 ".once.sql_FAILED"

*** 3.4 上传时全量覆盖之前的数据 (请小心操作)
有的时候, 我们需要在本次上传数据前, 提前清空掉之前的数据, 而用本次上传的数据全量覆盖 (类似于sql中的 truncate table, 但是table本身的schema是不会变化的)

这时, 需要对于该表的settings.json中增加一个 overwriteExistingData 参数
#+BEGIN_SRC javascript
{
    "table": {
        "columns": [
            {"name": "order_date", "isPrimaryKey": true},
            {"name": "customer_id", "isPrimaryKey": true}
        ],
        "overwriteExistingData": true
    }
}
#+END_SRC

** 4. 导入Excel/CSV文件
和 "3. GreenPlum 实际示例" 中类似, 在 csv/excel 子目录下, 也可以先新建一个子目录, 来代表要上传的数据Table的名字, 然后把之前的csv文件直接放到该目录下来马上导入该文件到观数
** 5. 用Card内容作为数据源
和 "3. GreenPlum实际示例"类似, 支持马上导入 card内容(once), 和定时调度 (schedule)

同样需要配置主键(去重)等.

区别:
  1. 目录需要在 Guandata_Workbench/card/目录下. 同样新建该数据源的子目录
  2. 马上执行的任务是  your_card_id.once.card  文件, 文件内容为空就行.  需要把 `your_card_id` 替换为你的已经存在的card
  3. 对于定时调度. 可能需要配置另一个 schedule trigger, 比如: 原始一般导入文件都是 "daily" 这个trigger下(比如: 早上5点调度).  则可以新建一个trigger "daily_plus_1_hour", 让其在6点钟执行, 所以, 这时, 实际的空文件名字为:   your_card_id.daily_plus_1_hour.card

** 6. FAQ
*** FAQ 1. 如果我有多个 GreenPlum 数据库, 怎么办?
Q: 因为 Guandata_Workbench 下的settings.json文件中, 只能配置一个 GreenPlum 数据库, 那我要是有多个数据库要导入怎么办?

A: 把最常用的数据库信息放在原来的 settings.json 中, 对于其它的数据库, 在其数据表的子目录中, 比如: "Guandata_Workbench/greenplum/每天订单分析" 下的 settings.json 中, 加入以下信息 ("table"相关信息是之前就有的, 本次加入的是 greenplum 中的内容):
#+BEGIN_SRC javascript
{
    "table": {
        "columns": [
            {"name": "order_date", "isPrimaryKey": true},
            {"name": "customer_id", "isPrimaryKey": true}
        ]
    },
    "greenplum": {
        "url": "jdbc:postgresql://127.0.0.1:5432/new_database2",
        "username": "__change_to_your_db_user_name__",
        "password": "__change_to_your_db_user_password__"
  }
}
#+END_SRC
运行时, 数据表目录下的 settings.json 和 根目录下的 settings.json 会合并起来查找配置信息 (数据表下的文件优先级更高些)
